[
["index.html", "Nutrient Diagnosis Of Potato Chapter 1 Data processing 1.1 Data sets 1.2 Arranging the data frame 1.3 Map of experimental sites locations", " Nutrient Diagnosis Of Potato zcoulibali 2019-05-31 Chapter 1 Data processing 1.1 Data sets We need package tidyverse for data handling, DBI and RSQLite packages to make connexion and extract usefull tables in the potato historical database file pomme_de_terre.db. library(&quot;tidyverse&quot;) # loads dplyr &amp; ggplot2 library(&quot;DBI&quot;) # Database Interface for R library(&quot;RSQLite&quot;) # SQLite Interface for R db &lt;- dbConnect(SQLite(), dbname = &quot;data/pomme_de_terre.db&quot;) # Connect to potato database Select tables in the created connexion. data_df &lt;- dbReadTable(db, &quot;MetaData&quot;) %&gt;% left_join(dbReadTable(db, &quot;FoliarAnalysis&quot;), by=&#39;NoEssai&#39;) %&gt;% left_join(dbReadTable(db, &quot;TreatmentVariable&quot;), by=c(&#39;NoEssai&#39;, &#39;NoBloc&#39;, &#39;NoTraitement&#39;)) %&gt;% left_join(dbReadTable(db, &quot;MaturityOrder&quot;), by = &#39;Cultivar&#39;) %&gt;% as_tibble() Load recent collected trials data from the project experiments and the Quebec Ministry of Agriculture, Fisheries and Food (MAPAQ) trials. trials_df &lt;- read.csv2(&quot;data/donneesEssaisPdt.csv&quot;, sep = &#39;;&#39;, dec = &#39;.&#39;) Select usefull variables Select usefull columns for computations, with macroelements N, P, K, Mg, Ca only because oligoelements have too much missing data. The year is not needed instead it permits to know how long ago expériements have been monitored. Geographical coordinates are used to map sites locations. keys_col &lt;- c(&#39;NoEssai&#39;, &#39;NoBloc&#39;, &#39;NoTraitement&#39;) macro &lt;- c(&quot;AnalyseFoliaireN&quot;,&quot;AnalyseFoliaireP&quot;,&quot;AnalyseFoliaireK&quot;, &quot;AnalyseFoliaireCa&quot;,&quot;AnalyseFoliaireMg&quot;) coord_year &lt;- c(&#39;Annee&#39;, &#39;LatDD&#39;, &#39;LonDD&#39;) cult_yield &lt;- c(&#39;Cultivar&#39;, &#39;Maturity5&#39;, &#39;RendVendable&#39;) usefull_col &lt;- c(keys_col, macro, coord_year, cult_yield, &#39;AnalyseFoliaireStade&#39;) macro_elts &lt;- c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Mg&quot;) # for simplicity Reduced and joined data frame becomes fol_df the foliar data frame: data_df &lt;- data_df %&gt;% select(usefull_col) trials_df &lt;- trials_df %&gt;% select(usefull_col) fol_df &lt;- rbind(data_df, trials_df) colnames(fol_df)[which(names(fol_df) %in% macro)] &lt;- macro_elts glimpse(fol_df) ## Observations: 12,991 ## Variables: 15 ## $ NoEssai &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ NoBloc &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,... ## $ NoTraitement &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6,... ## $ N &lt;dbl&gt; 3.805, 3.356, 3.657, 3.610, 4.983, 4.277,... ## $ P &lt;dbl&gt; 0.22, 0.20, 0.22, 0.18, 0.22, 0.21, 0.28,... ## $ K &lt;dbl&gt; 6.31, 6.85, 6.44, 5.36, 5.92, 6.73, 5.75,... ## $ Ca &lt;dbl&gt; 1.69, 1.31, 1.13, 1.30, 1.23, 1.38, 1.17,... ## $ Mg &lt;dbl&gt; 0.53, 0.21, 0.32, 0.50, 0.44, 0.45, 0.51,... ## $ Annee &lt;int&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2003,... ## $ LatDD &lt;dbl&gt; 46.75306, 46.75306, 46.75306, 46.75306, 4... ## $ LonDD &lt;dbl&gt; -72.33861, -72.33861, -72.33861, -72.3386... ## $ Cultivar &lt;chr&gt; &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Goldrush&quot;, &quot;Gold... ## $ Maturity5 &lt;chr&gt; &quot;mid-season&quot;, &quot;mid-season&quot;, &quot;mid-season&quot;,... ## $ RendVendable &lt;dbl&gt; 18.9442, 40.3518, 33.0379, 37.5505, 46.00... ## $ AnalyseFoliaireStade &lt;chr&gt; &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;1... 1.2 Arranging the data frame Set trial number as factor fol_df$NoEssai &lt;- as.factor(fol_df$NoEssai) Choose cultivar Goldrush as reference, it as the maximum number of observations in the data frame. fol_df$Cultivar &lt;- relevel(factor(fol_df$Cultivar), ref = &quot;Goldrush&quot;) Relevel categorical maturity order. fol_df$Maturity5 &lt;- ordered(fol_df$Maturity5, levels = c(&quot;early&quot;,&quot;early mid-season&quot;,&quot;mid-season&quot;,&quot;mid-season late&quot;,&quot;late&quot;)) Leaf ionome with macroelements leafIonome &lt;- fol_df[macro_elts] Some custom functions are used for compositional analysis. The libraries robCompositions, compositions and Amelia are used for robust imputation with kNN (long process), compositional transformations and to portrait missing values respectively. source(&#39;https://raw.githubusercontent.com/essicolo/AgFun/master/ilrNA.R&#39;) source(&#39;https://raw.githubusercontent.com/essicolo/AgFun/master/ilrDefinition.R&#39;) source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/codadend2.R&quot;) library(&quot;robCompositions&quot;) # impCoda &amp; impKNNa: for data imputation library(&quot;compositions&quot;) # for ILR transformations: acomp, ilr, ilrInv require(&#39;Amelia&#39;) # Portrait of missing values Portrait of missing macroelements missmap(leafIonome) Figure 1.1: Portrait of missing macroelements. Impute missing data for samples (rows) with less than 3 missing elements among the five. The next cell initializes this computation codes. # keep track of empty rows: fol_df$leafIonome_allNA &lt;- apply(fol_df[macro_elts], 1, function(X) all(is.na(X))) # keep track of rows where there is any NA: fol_df$leafIonome_anyNA &lt;- apply(fol_df[macro_elts], 1, function(X) any(is.na(X))) # number of NAs (missing values): fol_df$leafIonome_countNA &lt;- apply(fol_df[macro_elts], 1, function(X) sum(is.na(X))) # Only impute if the next variable value is set to FALSE fol_df$leafIonome_hasTooMuchNA &lt;- fol_df$leafIonome_countNA &gt;= 3 Imputation: # Warning: could be a long process leaf_imputeRob &lt;- impKNNa(as.matrix(fol_df[!fol_df$leafIonome_hasTooMuchNA, macro_elts]), metric = &quot;Aitchison&quot;, k = 6, primitive = TRUE, normknn = TRUE, adj = &#39;median&#39;) colnames(leaf_imputeRob$xImp) &lt;- paste0(colnames(leaf_imputeRob$xImp), &#39;_imp&#39;) leaf_imputeRob$xImp %&gt;% head() # a view of new imputed compositions ## N_imp P_imp K_imp Ca_imp Mg_imp ## [1,] 3.805 0.22 6.31 1.69 0.53 ## [2,] 3.356 0.20 6.85 1.31 0.21 ## [3,] 3.657 0.22 6.44 1.13 0.32 ## [4,] 3.610 0.18 5.36 1.30 0.50 ## [5,] 4.983 0.22 5.92 1.23 0.44 ## [6,] 4.277 0.21 6.73 1.38 0.45 Push imputed columns to the data frame. The nutrients diagnosis will be performed with imputed compositions. fol_df &lt;- left_join(x = fol_df, y = data.frame(NoEssai = fol_df$NoEssai[!fol_df$leafIonome_hasTooMuchNA], NoBloc = fol_df$NoBloc[!fol_df$leafIonome_hasTooMuchNA], NoTraitement = fol_df$NoTraitement[!fol_df$leafIonome_hasTooMuchNA], leaf_imputeRob$xImp), by = keys_col) fol_df &lt;- fol_df %&gt;% select(-c(&quot;leafIonome_allNA&quot;, &quot;leafIonome_anyNA&quot;, &quot;leafIonome_countNA&quot;, &quot;leafIonome_hasTooMuchNA&quot;)) Compute Fv. Fv stands for filling value, an amalgamation of all other elements closing the simplex to 100%. fol_df$Fv_imp &lt;- 100 - rowSums(fol_df[, colnames(leaf_imputeRob$xImp)]) # computes the Fv leaf.macro_imp &lt;- c(colnames(leaf_imputeRob$xImp), &#39;Fv_imp&#39;) # new colnames vector of macroelements leaf.macro_imp ## [1] &quot;N_imp&quot; &quot;P_imp&quot; &quot;K_imp&quot; &quot;Ca_imp&quot; &quot;Mg_imp&quot; &quot;Fv_imp&quot; Compute centered log-ratios clr only for complete cases. leafIonomeComp &lt;- acomp(fol_df[leaf.macro_imp]) leafIonomeClr &lt;- clr(leafIonomeComp) leafIonomeClr[apply(leafIonomeComp, 1, anyNA), ] &lt;- NA # discard for rows with any NA leafIonomeDefClr &lt;- paste0(&quot;clr_&quot;, c(macro_elts, &#39;Fv&#39;)) # computed clr colnames vector colnames(leafIonomeClr) &lt;- leafIonomeDefClr leafIonomeClr &lt;- cbind(fol_df[keys_col], leafIonomeClr) # bind clr variables to keys columns Push clr coordinates to fol_df and reduce the data frame to usefull columns fol_df &lt;- left_join(fol_df, leafIonomeClr, by = keys_col) fol_df &lt;- fol_df %&gt;% select(keys_col, coord_year, cult_yield, &quot;AnalyseFoliaireStade&quot;, leafIonomeDefClr) glimpse(fol_df) ## Observations: 12,991 ## Variables: 16 ## $ NoEssai &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,... ## $ NoBloc &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,... ## $ NoTraitement &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6,... ## $ Annee &lt;int&gt; 2003, 2003, 2003, 2003, 2003, 2003, 2003,... ## $ LatDD &lt;dbl&gt; 46.75306, 46.75306, 46.75306, 46.75306, 4... ## $ LonDD &lt;dbl&gt; -72.33861, -72.33861, -72.33861, -72.3386... ## $ Cultivar &lt;fct&gt; Goldrush, Goldrush, Goldrush, Goldrush, G... ## $ Maturity5 &lt;ord&gt; mid-season, mid-season, mid-season, mid-s... ## $ RendVendable &lt;dbl&gt; 18.9442, 40.3518, 33.0379, 37.5505, 46.00... ## $ AnalyseFoliaireStade &lt;chr&gt; &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;10% fleur&quot;, &quot;1... ## $ clr_N &lt;dbl&gt; 0.3321186, 0.4252302, 0.4453417, 0.399326... ## $ clr_P &lt;dbl&gt; -2.518325, -2.394957, -2.365429, -2.59918... ## $ clr_K &lt;dbl&gt; 0.83793831, 1.13872910, 1.01122715, 0.794... ## $ clr_Ca &lt;dbl&gt; -0.4794688, -0.5154924, -0.7290838, -0.62... ## $ clr_Mg &lt;dbl&gt; -1.639076, -2.346167, -1.990736, -1.57752... ## $ clr_Fv &lt;dbl&gt; 3.466813, 3.692658, 3.628680, 3.604817, 3... Cultivars classes correction. Cultivar Mystere and Vivaldi have different maturity classes in the data set, mid-season late and late for Mystere, then early mid-season and mid-season for Vivaldi. Their new maturity classes names are based on a majority vote. fol_df$Maturity5[fol_df$Cultivar == &quot;Mystere&quot;] &lt;- &quot;late&quot; fol_df$Maturity5[fol_df$Cultivar == &quot;Vivaldi&quot;] &lt;- &quot;early mid-season&quot; fol_df$Cultivar &lt;- forcats::fct_explicit_na(fol_df$Cultivar) Summarise the data frame. fol_df %&gt;% summarise(start_year = min(Annee, na.rm = TRUE), end_year = max(Annee, na.rm = TRUE), nbr_trials = n_distinct(NoEssai, na.rm = TRUE), nbr_cultivars = n_distinct(Cultivar, na.rm = TRUE), nbr_maturityClass = n_distinct(Maturity5, na.rm = TRUE) ) ## # A tibble: 1 x 5 ## start_year end_year nbr_trials nbr_cultivars nbr_maturityClass ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1958 2017 4930 60 5 Backup for cluster analysis: write.csv2(fol_df, &#39;data/fol_df.csv&#39;) 1.3 Map of experimental sites locations library(&quot;ggmap&quot;) # maps with ggplot2 library(&quot;extrafont&quot;) # Changing Fonts for Graphs qc_fol &lt;- get_stamenmap(bbox = c(left=-76, right=-68, bottom=45, top=50), zoom=7, maptype = &#39;toner-lite&#39;) ggmap(qc_fol) + geom_point(data=unique(fol_df[c(&#39;LonDD&#39;, &#39;LatDD&#39;)]), aes(x=LonDD, y=LatDD), size=3, shape=16, colour=&#39;aquamarine4&#39;, alpha=0.8) + coord_map(&quot;mercator&quot;) + theme_bw() + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) Figure 1.2: Location of experimental sites (green dots) in the Québec potato data set. #ggsave(&quot;images/fol-locations.png&quot;, width=10, height=8) "],
["Chapter-Clustering.html", "Chapter 2 Cluster analysis of potato cultivars 2.1 Load and handle data 2.2 clr centroids computation 2.3 Axis reduction with LDA Linear Discriminant Analysis 2.4 K-means clustering using clr centroïds for high yielders 2.5 clr and ionomics groups interactions effects", " Chapter 2 Cluster analysis of potato cultivars 2.1 Load and handle data We need a set of packages for data handling. Others will be loaded whenever needed. library(&quot;tidyverse&quot;) # loads dplyr &amp; ggplot2 library(&#39;ellipse&#39;) # plot ellipses library(&quot;mvoutlier&quot;) # sign1, multivariate outliers detection library(&quot;ade4&quot;) # discriminant analysis library(&quot;vegan&quot;) # data clustering library(&quot;extrafont&quot;) # Changing Fonts for Graphs We will also use a custom function for discriminant analysis plots. source(&#39;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_trad.R&#39;) source(&quot;https://raw.githubusercontent.com/essicolo/AgFun/master/plotDA_gg.R&quot;) Load data file from previous 1.0_data preprocessing.Rmd codes. fol_df &lt;- read.csv2(&quot;data/fol_df.csv&quot;) # Key columns selection keys_col &lt;- c(&#39;NoEssai&#39;, &#39;NoBloc&#39;, &#39;NoTraitement&#39;) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Mg&quot;, &quot;clr_Ca&quot;, &quot;clr_Fv&quot;) cult_yield &lt;- c(&#39;Cultivar&#39;, &#39;Maturity5&#39;, &#39;AnalyseFoliaireStade&#39;, &#39;RendVendable&#39;) For cluster analysis, keep only high yielders, i.e. yield 65% quantile cutter for each cultivar. The cutQ table contains the yield delimiter for each cultivar. cutQ &lt;- fol_df %&gt;% group_by(Cultivar) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, quantile, probs=0.65, na.rm = TRUE) %&gt;% rename(rv_cut = RendVendable) The cutQ table is used to add the variable yieldClass to fol_df. fol_df &lt;- fol_df %&gt;% left_join(cutQ, by = &quot;Cultivar&quot;) %&gt;% mutate(yieldClass = fct_relevel(ifelse(RendVendable &gt;= rv_cut, &quot;HY&quot;, &quot;LY&quot;), &quot;LY&quot;)) For sake of verification, we compute average yield per yieldClass. meanYield = fol_df %&gt;% group_by(yieldClass) %&gt;% select(RendVendable) %&gt;% summarise_if(is.numeric, mean, na.rm = TRUE) ## Adding missing grouping variables: `yieldClass` meanYield ## # A tibble: 3 x 2 ## yieldClass RendVendable ## &lt;fct&gt; &lt;dbl&gt; ## 1 LY 24.8 ## 2 HY 40.4 ## 3 &lt;NA&gt; NaN 2.2 clr centroids computation Compositional data transformation is done in the loaded file. We keep only clr-transformed coordinates for high yielders, at 10 % blossom (AnalyseFoliaireStade = 10% fleur). highYielders_df &lt;- fol_df %&gt;% mutate(isNA = apply(.[c(clr_no, cult_yield)], 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; yieldClass == &quot;HY&quot; &amp; NoEssai != &quot;2&quot;) %&gt;% select(one_of(keys_col, clr_no, cult_yield)) %&gt;% droplevels() nrow(highYielders_df) ## [1] 1401 So, 1401 lines of observations (or samples) will be used for clustering. Check how many rows of data you have for each cultivar: percentage &lt;- round(with(highYielders_df, prop.table(table(Cultivar)) * 100), 2) distribution &lt;- with(highYielders_df, cbind(nHY = table(Cultivar), percent = percentage)) distribution &lt;- data.frame(cbind(distribution, rownames(distribution))) colnames(distribution)[3] &lt;- &quot;Cultivar&quot; distribution$nHY &lt;- as.numeric(as.character(distribution$nHY)) # nHY = number of samples distribution$percent &lt;- as.numeric(as.character(distribution$percent)) # percentage distribution %&gt;% arrange(desc(nHY)) %&gt;% head(10) # arrange in descending order ## nHY percent Cultivar ## 1 233 16.63 Goldrush ## 2 203 14.49 Superior ## 3 158 11.28 FL 1207 ## 4 132 9.42 Chieftain ## 5 88 6.28 Atlantic ## 6 74 5.28 Kennebec ## 7 65 4.64 FL 1533 ## 8 58 4.14 Coastal Russet ## 9 51 3.64 Snowden ## 10 27 1.93 Mystere Some cultivars are well represented, like Goldrush and Superior. Let’s compute numbers of cultivars and trials for high yielders. data.frame(nbr_cultivars = n_distinct(highYielders_df$Cultivar, na.rm = T), nbr_trials = n_distinct(highYielders_df$NoEssai, na.rm = T)) ## nbr_cultivars nbr_trials ## 1 45 149 A table with cultivars, maturity classes and median clr values (i.e., Centroids). highYielders_clr &lt;- highYielders_df %&gt;% group_by(Cultivar, Maturity5) %&gt;% select(Cultivar, Maturity5, starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) highYielders_clr ## # A tibble: 45 x 8 ## # Groups: Cultivar [45] ## Cultivar Maturity5 clr_N clr_P clr_K clr_Mg clr_Ca clr_Fv ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AC Belmont early 0.698 -2.00 0.662 -1.62 -1.12 3.46 ## 2 AC Chaleur early 0.688 -1.98 0.352 -1.54 -1.02 3.50 ## 3 Amandine mid-season 0.526 -2.30 0.525 -1.59 -0.680 3.51 ## 4 Ambra mid-season 0.713 -1.98 0.275 -1.39 -1.08 3.46 ## 5 Andover early mid-season 0.812 -2.17 0.749 -2.28 -0.808 3.61 ## 6 Aquilon mid-season 0.783 -1.86 0.387 -1.83 -0.901 3.48 ## 7 Argos late 0.767 -1.77 0.570 -1.70 -1.28 3.43 ## 8 Atlantic mid-season 0.784 -1.87 0.155 -1.50 -1.09 3.56 ## 9 Bijou Rouge early 0.942 -1.93 0.563 -1.93 -1.12 3.61 ## 10 Carolina early 0.598 -2.19 0.131 -1.34 -0.837 3.63 ## # ... with 35 more rows Identify outliers with a criterion of 0.975 by cultivar, if cultivars contain at leat 20 rows. If less than 20 rows, all rows are kept. The new data frame is used for discriminant analysis lda_df. highYielders_df_IO &lt;- highYielders_df %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% do({ if (nrow(.) &lt; 20) { IO = rep(1, nrow(.)) } else { IO = sign1(.[,-1], qcrit=0.975)$wfinal01 } cbind(.,IO) }) lda_df &lt;- highYielders_df_IO %&gt;% filter(IO == 1) %&gt;% droplevels() nrow(lda_df) ## [1] 1244 lda_df %&gt;% head() ## # A tibble: 6 x 8 ## # Groups: Cultivar [1] ## Cultivar clr_N clr_P clr_K clr_Mg clr_Ca clr_Fv IO ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 AC Belmont 0.708 -2.32 0.718 -1.75 -0.813 3.45 1 ## 2 AC Belmont 0.610 -2.35 0.730 -1.81 -0.739 3.56 1 ## 3 AC Belmont 0.727 -1.87 0.724 -1.68 -1.34 3.44 1 ## 4 AC Belmont 0.752 -1.89 0.657 -1.66 -1.28 3.42 1 ## 5 AC Belmont 0.610 -2.01 0.664 -1.59 -1.14 3.46 1 ## 6 AC Belmont 0.723 -1.96 0.616 -1.59 -1.22 3.42 1 2.3 Axis reduction with LDA Linear Discriminant Analysis lda_df$Cultivar &lt;- factor(lda_df$Cultivar) pca_fol &lt;- dudi.pca(lda_df[clr_no], scannf = FALSE, scale = FALSE) lda_fol &lt;- discrimin(dudi = pca_fol, fac = factor(lda_df$Cultivar), scannf = FALSE) lda_fol_score = lda_fol$li lda_fol_loading = lda_fol$fa lda_fol_group = lda_df$Cultivar n_cultivar = table(lda_df$Cultivar) # Do not schow Cultivars whose number of occurrences is &lt; 5 n_filter &lt;- lda_fol_group %in% names(n_cultivar[n_cultivar &gt;= 5]) filter_cultivars &lt;- names(n_cultivar[n_cultivar &gt;= 5]) lda_df_filter &lt;- lda_df[n_filter, ] The distance biplot, a first view. options(repr.plot.width = 6, repr.plot.height = 6) plot_lda(score=lda_fol_score[n_filter, ], loading=lda_fol_loading[n_filter, ], group = lda_fol_group[n_filter], ell_dev=FALSE, ell_err= FALSE, #TRUE, scale_load = 0.5, level=0.95, legend=FALSE, label=TRUE, transparency=0.3, xlim = c(-2.5, 2), ylim = c(-3.5, 3.5), points=F) Figure 2.1: Discriminant distance biplot of potato cultivars. 2.4 K-means clustering using clr centroïds for high yielders The next data frame is the same as highYielders_clr without maturity classes. highYieldersCentroids &lt;- highYielders_df %&gt;% group_by(Cultivar) %&gt;% select(starts_with(&quot;clr&quot;)) %&gt;% summarise_all(list(median)) We use Calinski-Harabasz (1974) criterion (package vegan) for clustering. set.seed(194447) highYieldersKmeans &lt;- cascadeKM(highYieldersCentroids[, -1], inf.gr = 3, sup.gr = 8, criterion = &quot;calinski&quot;) Plot K-means clustering results. options(repr.plot.width = 6, repr.plot.height = 4) plot(highYieldersKmeans) Figure 2.2: K-means partitions comparison (calinski criterion). The red dot of the right hand side graph shows 4 optimal clustering partitions. Check the differnt partitions data frame. highYieldersKmeans$partition %&gt;% head() ## 3 groups 4 groups 5 groups 6 groups 7 groups 8 groups ## 1 3 3 4 3 7 3 ## 2 1 2 5 4 4 1 ## 3 2 1 1 5 6 2 ## 4 1 2 5 4 4 7 ## 5 3 4 3 6 2 5 ## 6 3 4 3 3 7 3 Consider 4 groups from clustering according to calinski criterion, this corresponds to the column 2 or use 4 groups as column name directly, and add it up to the previous data frame. highYieldersCentroids$kgroup &lt;- highYieldersKmeans$partition[, &quot;4 groups&quot;] Compute discriminant scores centroïdes for cultivars. lda_centroids &lt;- lda_fol_score %&gt;% mutate(group = lda_fol_group) %&gt;% group_by(group) %&gt;% summarise_all(list(mean)) Plot Cultivar groups in LDA with a custom function. options(repr.plot.width = 6, repr.plot.height = 6) #png(&quot;images/clustering.png&quot;, width=3000, height=1400, res = 300) plot_lda(score=lda_fol_score[n_filter, ], loading=lda_fol_loading[n_filter, ], group = lda_fol_group[n_filter], ell_dev=FALSE, ell_err= FALSE, #TRUE, scale_load = 0.4, level=0.95, legend=FALSE, label=TRUE, transparency=0.4, xlim = c(-2.5, 2), ylim = c(-3.5, 3.5), points=F) coll = factor(highYieldersCentroids[highYieldersCentroids$Cultivar %in% filter_cultivars, &#39;kgroup&#39;][[1]]) # remove the cultivar column points(lda_centroids[lda_centroids$group %in% filter_cultivars, c(&#39;DS1&#39;, &#39;DS2&#39;)], pch = 19, col = coll, # colours dots for groups or clusters. cex = 0.9) legend(-2.5, 3, legend = paste(rep(&#39;cluster&#39;, nlevels(coll)), as.numeric(levels(coll))), pch = 19, col = unique(coll), cex = 0.9) Figure 2.3: Discriminant distance biplot of potato cultivars showing ionomics groups. It’s a bit difficult to colour cultivar names in the plot with the custom function. Use functions from packages ggplot2, ggrepel and plotly instead. The package ggplot2 is already loaded with tidyverse. New data frames are created with useful variables. library(&quot;ggrepel&quot;) library(&quot;plotly&quot;) cultivars_filtre &lt;- data.frame(Cultivar = filter_cultivars, i_group=coll) df &lt;- data.frame( score = lda_fol_score[n_filter, ], loadings = lda_fol_loading[n_filter,], Cultivar = lda_fol_group[n_filter] ) df &lt;- df %&gt;% left_join(cultivars_filtre, by=&quot;Cultivar&quot;) df %&gt;% head() ## score.DS1 score.DS2 loadings.DS1 loadings.DS2 Cultivar i_group ## 1 -0.66902049 1.2508355 -1.80452082 -0.8166182 AC Belmont 3 ## 2 -0.55105954 0.8275579 0.33929124 1.1542930 AC Belmont 3 ## 3 -0.40168546 0.9054121 -1.96450084 2.3374270 AC Belmont 3 ## 4 -0.30280178 0.9424118 2.28179831 0.8360123 AC Belmont 3 ## 5 0.10410599 1.0195041 -0.04728401 1.8852473 AC Belmont 3 ## 6 -0.02339004 0.9371925 1.19521611 -5.3963614 AC Belmont 3 centroids = lda_centroids[lda_centroids$group %in% filter_cultivars, ] names(centroids)[match(&quot;group&quot;, names(centroids))] &lt;- &quot;Cultivar&quot; centroids &lt;- centroids %&gt;% left_join(cultivars_filtre, by=&quot;Cultivar&quot;) centroids %&gt;% head() ## # A tibble: 6 x 4 ## Cultivar DS1 DS2 i_group ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 AC Belmont 0.0326 0.787 3 ## 2 AC Chaleur 0.755 0.257 2 ## 3 Amandine 0.636 0.989 1 ## 4 Andover -1.90 0.138 4 ## 5 Aquilon -0.140 0.302 4 ## 6 Argos -0.119 0.677 3 Plot with ggplot2 #png(&quot;data/clustering.png&quot;, width=3000, height=1400, res = 300) options(repr.plot.width = 9, repr.plot.height = 9) g &lt;- ggplot(centroids, aes(DS1, DS2, label = Cultivar, col=i_group)) + geom_text_repel() + geom_point(alpha = 0.5) + theme_classic(base_size = 12) + scale_color_manual(values=c(&quot;red&quot;, &quot;magenta&quot;, &quot;blue&quot;, &quot;black&quot;)) + theme(axis.text=element_text(size=12)) + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) # Add discriminant loadings using geom_segment and arrow x=0; y=0; labels = c(clr_no, rep(NA, nrow(df)-length(clr_no))) g + geom_segment(data=df, mapping=aes(x=x, y=y, xend=x+loadings.DS1, yend=y+loadings.DS2), arrow=arrow(), size = 1, color=&quot;grey80&quot;) + geom_text(data=df, mapping=aes(x=loadings.DS1, y=loadings.DS2, label=labels), size=5, color=&quot;black&quot;) + geom_hline(yintercept=0, color=&quot;black&quot;, linetype=2) + geom_vline(mapping=aes(xintercept=0), color=&quot;black&quot;, linetype=2) + theme(axis.line=element_blank()) Figure 2.4: Discriminant biplot and cluster analysis result of potato cultivars. #ggsave(&quot;images/cultivar_clust.png&quot;, width=10, height=8, dpi = 300) Push Cultivars yield cut-off and ionomics groups in the data frame. ionomicGroup &lt;- data.frame(lda_centroids[, 1], ionomicGroup = factor(highYieldersKmeans$partition[, &quot;4 groups&quot;])) colnames(ionomicGroup)[colnames(ionomicGroup)==&quot;group&quot;] &lt;- &quot;Cultivar&quot; cutQ &lt;- cutQ[-1, ] # to discard missing cultivars colnames(cutQ)[which(names(cutQ) == &quot;rv_cut&quot;)] &lt;- &quot;yieldCutoff&quot; cutQ_ig &lt;- cutQ %&gt;% left_join(ionomicGroup, by = &quot;Cultivar&quot;) fol_df &lt;- fol_df %&gt;% left_join(y = cutQ_ig, by = &#39;Cultivar&#39;) %&gt;% select(-rv_cut) Processing data for Machine Learning dfml, saved as data_ionome.csv. Exctract usefull columns from fol_df. Conserve only `complete cases. dfml &lt;- fol_df %&gt;% mutate(isNA = apply(.[c(clr_no, cult_yield)], 1, anyNA)) %&gt;% mutate(is10pcf = AnalyseFoliaireStade == &quot;10% fleur&quot;) %&gt;% filter(!isNA &amp; is10pcf &amp; NoEssai != &quot;2&quot;) %&gt;% select(one_of(c(keys_col, clr_no, cult_yield, &#39;yieldClass&#39;, &#39;ionomicGroup&#39;))) %&gt;% select(-AnalyseFoliaireStade) %&gt;% droplevels() %&gt;% filter(complete.cases(.)) nrow(dfml) ## [1] 3369 Backup write.csv2(dfml, &quot;data/data_ionome.csv&quot;) 2.5 clr and ionomics groups interactions effects Linear mixed effect modeling of yield relative to the ionome*ionomicGroup interaction (extraction of the interactions coefficients). library(&quot;nlme&quot;) source(&quot;data/functions.R&quot;) # contains a costum r-square function dfml$Cultivar &lt;- factor(dfml$Cultivar) dfml$Maturity5 &lt;- relevel(dfml$Maturity5, ref=&quot;late&quot;) dfml$Cultivar &lt;- relevel(dfml$Cultivar, ref=&quot;Goldrush&quot;) dfml$NoEssai &lt;- factor(dfml$NoEssai) colnames(dfml)[colnames(dfml)==&quot;ionomicGroup&quot;] &lt;- &quot;group_i&quot; dfml$group_i &lt;- factor(dfml$group_i) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Ca&quot;, &quot;clr_Mg&quot;, &quot;clr_Fv&quot;) clrNo &lt;- c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrCa&quot;, &quot;clrMg&quot;, &quot;clrFv&quot;) # for plot colnames(dfml)[which(names(dfml) %in% clr_no)] &lt;- clrNo Scale clr coordinates dfml.sc &lt;- dfml # copy dfml.sc[, clrNo] &lt;- apply(dfml.sc[, clrNo], 2, scale) Fit linear mixed model. Discard the filling value to deal with singularity problem. used_clr = c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrCa&quot;, &quot;clrMg&quot;) # without &quot;clr_Fv&quot; lmm &lt;- lme(RendVendable ~ (clrN + clrP + clrK + clrCa + clrMg):group_i, data=dfml.sc, random= ~1|NoEssai) pseudoR2 = rsq(dfml.sc$RendVendable, predict(lmm)) pseudoR2 ## [1] 0.7400044 Extract the interactions coefficients and their p-values (pv) matrix: pv &lt;- summary(lmm)$tTable[-1,] pv ## Value Std.Error DF t-value p-value ## clrN:group_i1 6.645143 2.1649322 3150 3.069446 2.162828e-03 ## clrN:group_i2 9.007241 0.6874520 3150 13.102357 3.132439e-38 ## clrN:group_i3 6.949603 1.4852004 3150 4.679236 3.000809e-06 ## clrN:group_i4 3.195431 0.3866881 3150 8.263587 2.057744e-16 ## clrP:group_i1 6.795965 2.7092875 3150 2.508396 1.217794e-02 ## clrP:group_i2 8.449904 0.8658023 3150 9.759623 3.449225e-22 ## clrP:group_i3 9.003016 2.2037071 3150 4.085396 4.509847e-05 ## clrP:group_i4 1.618988 0.4610944 3150 3.511187 4.523723e-04 ## clrK:group_i1 7.646919 2.8537539 3150 2.679600 7.409365e-03 ## clrK:group_i2 9.272317 0.9305423 3150 9.964423 4.767161e-23 ## clrK:group_i3 3.434110 2.7288719 3150 1.258436 2.083275e-01 ## clrK:group_i4 1.740554 0.6261693 3150 2.779686 5.473444e-03 ## clrCa:group_i1 7.936266 2.4551030 3150 3.232559 1.239518e-03 ## clrCa:group_i2 9.912541 0.9136017 3150 10.849958 5.942979e-27 ## clrCa:group_i3 7.201106 1.9860081 3150 3.625920 2.924953e-04 ## clrCa:group_i4 3.230313 0.5834249 3150 5.536811 3.333127e-08 ## clrMg:group_i1 7.332749 2.7405918 3150 2.675608 7.497989e-03 ## clrMg:group_i2 7.389987 0.8650038 3150 8.543300 2.000749e-17 ## clrMg:group_i3 4.020082 2.1795290 3150 1.844473 6.520812e-02 ## clrMg:group_i4 1.114686 0.4414678 3150 2.524955 1.162000e-02 Also extract their confident intervals, and process data for the plot. interval &lt;- tibble(Estimate = intervals(lmm)$fixed[-1, 2], LL = intervals(lmm)$fixed[-1, 1], UL = intervals(lmm)$fixed[-1, 3]) interval$variable &lt;- rep(&#39;NA&#39;, nrow(interval)) interval$variable &lt;- rownames(intervals(lmm)$fixed)[-1] interval$ionomic_group &lt;- rep(paste(&quot;group&quot;, 1:nlevels(dfml$group_i)), length(clrNo)-1) interval$used_clr &lt;- rep(used_clr, each = nlevels(dfml$group_i)) interval$pvalue &lt;- pv[,&quot;p-value&quot;] interval$is_significant = ifelse(interval$pvalue &lt;= 0.05, &#39;P &lt; 0.05&#39;, &#39;P &gt; 0.05&#39;) interval ## # A tibble: 20 x 8 ## Estimate LL UL variable ionomic_group used_clr pvalue ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6.65 2.40 10.9 clrN:gr~ group 1 clrN 2.16e- 3 ## 2 9.01 7.66 10.4 clrN:gr~ group 2 clrN 3.13e-38 ## 3 6.95 4.04 9.86 clrN:gr~ group 3 clrN 3.00e- 6 ## 4 3.20 2.44 3.95 clrN:gr~ group 4 clrN 2.06e-16 ## 5 6.80 1.48 12.1 clrP:gr~ group 1 clrP 1.22e- 2 ## 6 8.45 6.75 10.1 clrP:gr~ group 2 clrP 3.45e-22 ## 7 9.00 4.68 13.3 clrP:gr~ group 3 clrP 4.51e- 5 ## 8 1.62 0.715 2.52 clrP:gr~ group 4 clrP 4.52e- 4 ## 9 7.65 2.05 13.2 clrK:gr~ group 1 clrK 7.41e- 3 ## 10 9.27 7.45 11.1 clrK:gr~ group 2 clrK 4.77e-23 ## 11 3.43 -1.92 8.78 clrK:gr~ group 3 clrK 2.08e- 1 ## 12 1.74 0.513 2.97 clrK:gr~ group 4 clrK 5.47e- 3 ## 13 7.94 3.12 12.8 clrCa:g~ group 1 clrCa 1.24e- 3 ## 14 9.91 8.12 11.7 clrCa:g~ group 2 clrCa 5.94e-27 ## 15 7.20 3.31 11.1 clrCa:g~ group 3 clrCa 2.92e- 4 ## 16 3.23 2.09 4.37 clrCa:g~ group 4 clrCa 3.33e- 8 ## 17 7.33 1.96 12.7 clrMg:g~ group 1 clrMg 7.50e- 3 ## 18 7.39 5.69 9.09 clrMg:g~ group 2 clrMg 2.00e-17 ## 19 4.02 -0.253 8.29 clrMg:g~ group 3 clrMg 6.52e- 2 ## 20 1.11 0.249 1.98 clrMg:g~ group 4 clrMg 1.16e- 2 ## # ... with 1 more variable: is_significant &lt;chr&gt; Plot with ggplot2. options(repr.plot.width = 5, repr.plot.height = 5) gg &lt;- ggplot(data = interval, mapping = aes(x = Estimate, y = used_clr, color=is_significant)) + facet_grid(ionomic_group ~ .) + #, scales = &#39;free&#39;, space = &#39;free&#39;) + geom_vline(xintercept = 0, lty = 2) + geom_segment(mapping = aes(x = LL, xend = UL, y = used_clr, yend = used_clr)) + geom_point() + #scale_color_grey(start=.1, end=0) + labs(x = &quot;Coefficient&quot;, y = &quot;&quot;) + theme_bw() + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) gg + theme(legend.title = element_blank())#, legend.position = &#39;bottom&#39;) Figure 2.5: Effect of ionome perturbation on marketable yield as illustrated by a linear mixed effect model. #ggsave(&quot;images/coef_lmm.tiff&quot;, width = 5, height = 5, dpi = 300) "],
["Chapter-Modeling.html", "Chapter 3 Predicting marketable tuber yield 3.1 Load data file 3.2 Machine learning 3.3 Assessing goodness of fit", " Chapter 3 Predicting marketable tuber yield 3.1 Load data file Load data file data_ionome.csv saved from previous cluster analysis. library(&quot;tidyverse&quot;) # &#39;diplyr&#39; and &#39;ggplot2&#39; library(&#39;extrafont&#39;) # Changing Fonts for Graphs df = read.csv2(&#39;data/data_ionome.csv&#39;) colnames(df)[colnames(df)==&quot;ionomicGroup&quot;] &lt;- &quot;group_i&quot; # make simplier df$group_i = factor(df$group_i) clr_no &lt;- c(&quot;clr_N&quot;, &quot;clr_P&quot;, &quot;clr_K&quot;, &quot;clr_Mg&quot;, &quot;clr_Ca&quot;, &quot;clr_Fv&quot;) clrNo &lt;- c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrMg&quot;, &quot;clrCa&quot;, &quot;clrFv&quot;) colnames(df)[which(names(df) %in% clr_no)] &lt;- clrNo df.sc = df # copy df.sc[, clrNo] &lt;- apply(df.sc[, clrNo], 2, scale) # scale clr coordinates Dummy code maturity order and ionomic group. if(&quot;Maturity5&quot; %in% colnames(df.sc)) { df.sc$Maturity5 &lt;- model.matrix(~ordered(factor(df.sc$Maturity5)))[, 2] } if(&quot;group_i&quot; %in% colnames(df.sc)) { df.sc$group_i &lt;- model.matrix(~ordered(factor(df.sc$group_i)))[, 2] } Check the data frame structure. pc &lt;- round(with(df.sc, prop.table(table(Cultivar)) * 100), 2) dist &lt;- with(df.sc, cbind(freq = table(Cultivar), percentage = pc)) dist &lt;- data.frame(cbind(dist, rownames(dist))) colnames(dist)[3] &lt;- &quot;Cultivar&quot; dist$freq &lt;- as.numeric(as.character(dist$freq)) dist %&gt;% arrange(desc(freq)) %&gt;% head(10) # or discard the last part to see all. ## freq percentage Cultivar ## 1 560 16.62 Goldrush ## 2 367 10.89 Superior ## 3 346 10.27 FL 1207 ## 4 256 7.6 Chieftain ## 5 189 5.61 Kennebec ## 6 188 5.58 Snowden ## 7 184 5.46 Atlantic ## 8 183 5.43 FL 1533 ## 9 165 4.9 Coastal Russet ## 10 112 3.32 Shepody 3.2 Machine learning Partioning data in Train and Test (evaluation) sets. Load libraries for machine learning functions. library(&#39;caret&#39;) library(&#39;kknn&#39;) set.seed(853739) # random.org split_index &lt;- createDataPartition(df.sc$yieldClass, group = &quot;Cultivar&quot;, p = 0.75, list = FALSE, times = 1) train &lt;- df.sc[split_index, ] test &lt;- df.sc[-split_index, ] ## With clr coordinates ml_clr &lt;- c(clrNo, &#39;yieldClass&#39;) train_clr = train[, ml_clr] test_clr = test[, ml_clr] ## With clr and maturity classes ml_mc &lt;- c(clrNo, &#39;Maturity5&#39;, &#39;yieldClass&#39;) train_mc = train[, ml_mc] test_mc = test[, ml_mc] ## With clr and ionomic groups ml_grp &lt;- c(clrNo, &#39;group_i&#39;, &#39;yieldClass&#39;) train_grp = train[, ml_grp] test_grp = test[, ml_grp] The knn model will be trained on a grid. grid &lt;- expand.grid(kmax = c(7,9,12,15), # neighborhood distance = 1:2, # 1 for euclidean distance, 2 for Mahalannobis kernel = &quot;optimal&quot;) grid ## kmax distance kernel ## 1 7 1 optimal ## 2 9 1 optimal ## 3 12 1 optimal ## 4 15 1 optimal ## 5 7 2 optimal ## 6 9 2 optimal ## 7 12 2 optimal ## 8 15 2 optimal The models will be train with a 10-fold cross-validation (cv) based on accuracy as loss function. control &lt;- trainControl(method = &quot;cv&quot;, number = 10) metric &lt;- &quot;Accuracy&quot; Train Models: # a) Non-linear algorithm ## kNN set.seed(7) kknn_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) kknn_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) kknn_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;kknn&quot;, metric = metric, trControl = control, tuneGrid = grid) # b) Advanced algorithms ## SVM set.seed(7) svm_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;svmRadial&quot;, metric = metric, trControl = control) svm_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;svmRadial&quot;, metric = metric, trControl = control) svm_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;svmRadial&quot;, metric = metric, trControl = control) ## Random Forest set.seed(7) rf_clr &lt;- train(yieldClass ~., data = train_clr, method = &quot;rf&quot;, metric = metric, trControl = control) rf_mc &lt;- train(yieldClass ~., data = train_mc, method = &quot;rf&quot;, metric = metric, trControl = control) rf_grp &lt;- train(yieldClass ~., data = train_grp, method = &quot;rf&quot;, metric = metric, trControl = control) 3.3 Assessing goodness of fit Models results, to select the best model. # Summary results results &lt;- resamples(list(kknn_clr_solely = kknn_clr, kknn_clr_and_ionomicgroup = kknn_grp, kknn_clr_and_maturityclass = kknn_mc, svm_clr_solely = svm_clr, svm_clr_and_ionomicgroup = svm_grp, svm_clr_and_maturityclass = svm_mc, rf_clr_solely = rf_clr, rf_clr_and_ionomicgroup = rf_grp, rf_clr_and_maturityclass = rf_mc)) #summary(results) with dotplot() options(repr.plot.width = 5, repr.plot.height = 4) dotplot(results) Figure 3.1: Comparison of models accuracies at training. Check the best model at training, but only accuracies with test sets are used as models quality metric. models_acc &lt;- data.frame(Model = summary(results)$models, Accuracy = c(confusionMatrix(train_clr$yieldClass, predict(kknn_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(kknn_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(kknn_mc))$overall[1], confusionMatrix(train_clr$yieldClass, predict(svm_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(svm_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(svm_mc))$overall[1], confusionMatrix(train_clr$yieldClass, predict(rf_clr))$overall[1], confusionMatrix(train_grp$yieldClass, predict(rf_grp))$overall[1], confusionMatrix(train_mc$yieldClass, predict(rf_mc))$overall[1])) models_acc[order(models_acc[,&quot;Accuracy&quot;], decreasing = TRUE), ] ## Model Accuracy ## 8 rf_clr_and_ionomicgroup 0.9825880 ## 7 rf_clr_solely 0.9821923 ## 9 rf_clr_and_maturityclass 0.9821923 ## 1 kknn_clr_solely 0.8428967 ## 3 kknn_clr_and_maturityclass 0.8357736 ## 2 kknn_clr_and_ionomicgroup 0.8314207 ## 6 svm_clr_and_maturityclass 0.7099327 ## 5 svm_clr_and_ionomicgroup 0.7020182 ## 4 svm_clr_solely 0.6735259 Quality metrics with test (evaluation) set. predicted_kknn_clr &lt;- predict(kknn_clr, test_clr) predicted_kknn_mc &lt;- predict(kknn_mc, test_mc) predicted_kknn_grp &lt;- predict(kknn_grp, test_grp) predicted_svm_clr &lt;- predict(svm_clr, test_clr) predicted_svm_mc &lt;- predict(svm_mc, test_mc) predicted_svm_grp &lt;- predict(svm_grp, test_grp) predicted_rf_clr &lt;- predict(rf_clr, test_clr) predicted_rf_mc &lt;- predict(rf_mc, test_mc) predicted_rf_grp &lt;- predict(rf_grp, test_grp) #The best model tests_acc &lt;- data.frame(Model = summary(results)$models, Accuracy_on_test = c( confusionMatrix(test_clr$yieldClass, predicted_kknn_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_kknn_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_kknn_mc)$overall[1], confusionMatrix(test_clr$yieldClass, predicted_svm_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_svm_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_svm_mc)$overall[1], confusionMatrix(test_clr$yieldClass, predicted_rf_clr)$overall[1], confusionMatrix(test_grp$yieldClass, predicted_rf_grp)$overall[1], confusionMatrix(test_mc$yieldClass, predicted_rf_mc)$overall[1])) tests_acc[order(tests_acc[,&quot;Accuracy_on_test&quot;], decreasing = TRUE), ] ## Model Accuracy_on_test ## 2 kknn_clr_and_ionomicgroup 0.7185273 ## 9 rf_clr_and_maturityclass 0.7173397 ## 3 kknn_clr_and_maturityclass 0.7137767 ## 1 kknn_clr_solely 0.7030879 ## 8 rf_clr_and_ionomicgroup 0.7007126 ## 7 rf_clr_solely 0.6864608 ## 5 svm_clr_and_ionomicgroup 0.6686461 ## 6 svm_clr_and_maturityclass 0.6591449 ## 4 svm_clr_solely 0.6282660 Yield Class Prediction with kknn algorithm on test set Order prediction quality metric by cultivar with kknn combining clr values and new clustered variable (kknn_clr_and_ionomicgroup). test$ypred = predicted_kknn_grp # adds predictions to test set cultivar_acc &lt;- test %&gt;% group_by(Cultivar) %&gt;% do(Accuracy = as.numeric(confusionMatrix(.$yieldClass, .$ypred)$overall[1]), N_obs = as.numeric(nrow(.))) cultivar_acc$Accuracy &lt;- unlist(cultivar_acc$Accuracy) cultivar_acc$N_obs &lt;- unlist(cultivar_acc$N_obs) data = data.frame(subset(cultivar_acc, Accuracy&gt;0)) data[order(data[,&quot;Accuracy&quot;], decreasing=T), ] ## Cultivar Accuracy N_obs ## 2 AC Chaleur 1.0000000 6 ## 3 Amandine 1.0000000 2 ## 16 Frontier Russet 1.0000000 6 ## 22 Krantz 1.0000000 1 ## 25 Norland 1.0000000 14 ## 36 Sifra 1.0000000 1 ## 39 Vivaldi 1.0000000 5 ## 1 AC Belmont 0.8750000 8 ## 28 Prospect 0.8571429 7 ## 29 Reba 0.8333333 6 ## 37 Snowden 0.8333333 48 ## 6 Argos 0.8000000 5 ## 40 W 1386 0.8000000 5 ## 20 Kennebec 0.7843137 51 ## 10 Chieftain 0.7826087 69 ## 4 Andover 0.7777778 9 ## 11 Coastal Russet 0.7446809 47 ## 17 Goldrush 0.7279412 136 ## 13 Estima 0.7142857 7 ## 23 Mystere 0.7142857 14 ## 14 FL 1207 0.7051282 78 ## 15 FL 1533 0.7017544 57 ## 35 Shepody 0.6956522 23 ## 8 Bijou Rouge 0.6666667 6 ## 24 Nordonna 0.6666667 3 ## 27 Pommerelle 0.6666667 9 ## 32 Roko 0.6666667 6 ## 34 Russet Norkota 0.6666667 3 ## 38 Superior 0.6538462 78 ## 5 Aquilon 0.6363636 22 ## 12 Dark Red Chieftain 0.6250000 8 ## 7 Atlantic 0.6000000 50 ## 42 Yukon Gold 0.6000000 20 ## 9 Carolina 0.5000000 2 ## 19 Kanona 0.5000000 2 ## 21 Keuka Gold 0.5000000 2 ## 26 Pike 0.5000000 2 ## 30 Red Cloud 0.5000000 2 ## 33 Russet Burbank 0.5000000 6 ## 31 Red Maria 0.4000000 5 ## 41 Waneta 0.4000000 5 ## 18 Harmony 0.3333333 3 Check it with geom_segment(): options(repr.plot.width = 8, repr.plot.height = 4) ggplot(data, aes(reorder(Cultivar, Accuracy), Accuracy)) + geom_point(aes(color=cut(Accuracy, breaks = c(0, 0.70, 0.90, 1)))) + geom_segment(aes(x=Cultivar, xend=Cultivar, y=0, yend=Accuracy, color=cut(Accuracy, breaks = c(0, 0.70, 0.90, 1))), size=1) + xlab(&quot;Cultivar&quot;) + theme_bw() + theme(legend.title=element_blank(), axis.text.x=element_text(angle=90, hjust=1))+ theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) (#fig:accuracy_cultivar)Predictive accuracy for cultivars. #ggsave(&quot;images/cultivAcc.tiff&quot;, width=8, height=3) Run the Chi-square homogenity test to compare prediction with a non-informative classification consisting of an equal distribution of 50% successful and 50% unsuccessful cases, using kknn. cm &lt;- confusionMatrix(predicted_kknn_grp, test_grp$yieldClass) # confusion matrix cm$table ## Reference ## Prediction HY LY ## HY 209 96 ## LY 141 396 # Model&#39;s classification good_class &lt;- cm$table[1,1]+cm$table[2,2] # HY or LY and correctly predicted misclass &lt;- cm$table[1,2]+cm$table[2,1] # wrong classification ml_class &lt;- c(good_class, misclass) # Non-informative model total &lt;- sum(cm$table) # number of samples good_nim &lt;- 0.50 * total misclass_nim &lt;- 0.50 * total non_inf_model &lt;- c(good_nim, misclass_nim) # Matrix for chisquare test m &lt;- rbind(ml_class, non_inf_model) m ## [,1] [,2] ## ml_class 605 237 ## non_inf_model 421 421 # chisq.test khi2_test &lt;- chisq.test(m) khi2_test ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: m ## X-squared = 83.535, df = 1, p-value &lt; 2.2e-16 The null hypothesis for a non-informative classification is rejected after the chi-square homogeneity test. The train data set with predicted yield classes train_df backup will be used to test perturbation vector theory in the next file. pred_yield &lt;- predict(kknn_grp, train_grp) train_df = data.frame(cbind(df[split_index, ], pred_yield)) write.csv2(train_df, &quot;data/train_df.csv&quot;) nrow(train_df) ## [1] 2527 True Negatives (TN) specimens in this study are observations of the training data set with high yield (HY) and correctly predicted. Compute clr centroids for True Negatives with original clr values; use original data frame df. TNs = train_df[train_df$yieldClass == &#39;HY&#39; &amp; pred_yield == &#39;HY&#39;, ] nrow(TNs) ## [1] 813 The next cell computes clr balances Centroids for ionomic groups. TNmedianNorms &lt;- TNs %&gt;% group_by(group_i) %&gt;% select(clrNo) %&gt;% summarise_all(list(median)) ## Adding missing grouping variables: `group_i` TNmedianNorms ## # A tibble: 4 x 7 ## group_i clrN clrP clrK clrMg clrCa clrFv ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.604 -2.17 0.513 -1.70 -0.749 3.41 ## 2 2 0.782 -2.00 0.220 -1.50 -1.01 3.50 ## 3 3 0.845 -1.94 0.532 -1.79 -1.19 3.56 ## 4 4 0.790 -1.98 0.550 -1.90 -1.01 3.59 "],
["Chapter-Perturbation-vector.html", "Chapter 4 Perturbation vector theory 4.1 Dissimilarity index between compositions 4.2 Rebalancing a misbalanced sample by perturbation", " Chapter 4 Perturbation vector theory The first step is to compute a dissimilarity index according to the distance from the closest healthy point (the closest TN). Let’s load data set. library(&quot;tidyverse&quot;) # dplyr and ggplot2 library(extrafont) # Changing Fonts for Graphs train_df = read.csv2(&quot;data/train_df.csv&quot;) train_df &lt;- train_df %&gt;% select(-X.1, -X) # to discard extra columns train_df$group_i = factor(train_df$group_i) As considered in previous 1.2_machine learning.ipynb file, True Negatives (TN) specimens in this study are observations of the training data set with high yield (HY) and correctly predicted. TNs = train_df[train_df$yieldClass == &#39;HY&#39; &amp; train_df$pred_yield == &#39;HY&#39;, ] clrNo = c(&quot;clrN&quot;, &quot;clrP&quot;, &quot;clrK&quot;, &quot;clrCa&quot;, &quot;clrMg&quot;, &quot;clrFv&quot;) # for simplicity 4.1 Dissimilarity index between compositions Euclidean distance as dissimilarity index For each composition (line) in the “unbalanced specimens”, calculate all the euclidean distances between all the compositions in “TNs” of the corresponding group Return the smallest euclidean distance as the unbalanced index of the composition. Function to compute euclidean distance eucl_dist_f &lt;- function(x, y) { sqrt(sum((x-y)^2)) } Compute debalance (or imbalance) index (debal) using this loop. debal &lt;- c() debal_index &lt;- c() for (i in 1:nrow(train_df)) { clr_i &lt;- as.numeric(train_df[i, clrNo]) eucl_dist &lt;- apply(TNs %&gt;% filter(group_i == train_df[i, &#39;group_i&#39;]) %&gt;% select(clrNo), 1, function(x) eucl_dist_f(x=x, y=clr_i)) debal_index[i] &lt;- which.min(eucl_dist) debal[i] &lt;- eucl_dist[debal_index[i]] } train_df$debal &lt;- debal train_df %&gt;% glimpse() ## Observations: 2,527 ## Variables: 16 ## $ NoEssai &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ NoBloc &lt;int&gt; 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3... ## $ NoTraitement &lt;int&gt; 1, 3, 5, 6, 7, 8, 1, 2, 3, 5, 2, 3, 4, 5, 6, 7, 8... ## $ clrN &lt;dbl&gt; 0.3321186, 0.4453417, 0.6519389, 0.4885647, 0.720... ## $ clrP &lt;dbl&gt; -2.518325, -2.365429, -2.468221, -2.525335, -2.30... ## $ clrK &lt;dbl&gt; 0.8379383, 1.0112272, 0.8242433, 0.9418880, 0.720... ## $ clrMg &lt;dbl&gt; -1.639076, -1.990736, -1.775074, -1.763195, -1.70... ## $ clrCa &lt;dbl&gt; -0.4794688, -0.7290838, -0.7470790, -0.6426036, -... ## $ clrFv &lt;dbl&gt; 3.466813, 3.628680, 3.514191, 3.500681, 3.432323,... ## $ Cultivar &lt;fct&gt; Goldrush, Goldrush, Goldrush, Goldrush, Goldrush,... ## $ Maturity5 &lt;fct&gt; mid-season, mid-season, mid-season, mid-season, m... ## $ RendVendable &lt;dbl&gt; 18.94420, 33.03790, 46.00890, 41.01670, 46.79370,... ## $ yieldClass &lt;fct&gt; LY, LY, HY, HY, HY, HY, LY, LY, HY, HY, LY, HY, H... ## $ group_i &lt;fct&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4... ## $ pred_yield &lt;fct&gt; LY, HY, HY, HY, HY, HY, LY, HY, HY, HY, HY, HY, H... ## $ debal &lt;dbl&gt; 0.1872777, 0.1588655, 0.0000000, 0.0000000, 0.000... 4.2 Rebalancing a misbalanced sample by perturbation Suppose we got this point selected at random in unbalanced specimens. #set.seed(932559) # random.org unbalanced &lt;- subset(train_df, debal !=0) misbalanced &lt;- unbalanced[sample(nrow(unbalanced), 1), ] misbalanced ## NoEssai NoBloc NoTraitement clrN clrP clrK clrMg ## 1541 196 2 12 0.8039553 -3.542755 0.7991936 -1.704475 ## clrCa clrFv Cultivar Maturity5 RendVendable yieldClass ## 1541 0.07201663 3.572065 Superior early mid-season 31.14754 LY ## group_i pred_yield debal ## 1541 4 LY 1.121824 Or even, you can use the most unbalanced occurrence, … why not ! (misbalanced = unbalanced[which.max(unbalanced$debal), ]) ## NoEssai NoBloc NoTraitement clrN clrP clrK clrMg ## 1605 200 1 2 0.957698 -1.83199 -0.04637332 -2.247862 ## clrCa clrFv Cultivar Maturity5 RendVendable yieldClass ## 1605 0.4678508 2.700677 Superior early mid-season 30.37194 LY ## group_i pred_yield debal ## 1605 4 LY 1.211534 How to rebalance it? The first step is to find in TNs of the corresponding ionomic group the closest balanced point. Let’s re-compute the euclidean distance, like for a new (an unknown) point: misbalanced &lt;- misbalanced[clrNo] eucl_dist_misbal &lt;- apply(TNs[, clrNo], 1, function(x) eucl_dist_f(x=x, y=misbalanced)) index_misbal &lt;- which.min(t(data.frame(eucl_dist_misbal))) # return the index of the sample index_misbal ## [1] 479 Euclidean distance matched with the corresponding debal value: misbal = eucl_dist_misbal[index_misbal] # to compare to corresponding debal value misbal ## 1540 ## 1.211534 The closest point in the TNs data set is this one: closest &lt;- TNs[index_misbal, ] closest ## NoEssai NoBloc NoTraitement clrN clrP clrK clrMg ## 1540 196 2 11 0.5819154 -2.201392 0.4035754 -1.854012 ## clrCa clrFv Cultivar Maturity5 RendVendable yieldClass ## 1540 -0.2247718 3.294685 Superior early mid-season 32.56831 HY ## group_i pred_yield ## 1540 4 HY Note that Ionomics groups of the misbalanced and the closest composition are the same … most of the times ! You need package compositions for further clr back-transformation de compositional space. require(&#39;compositions&#39;) Perturbation in compositional space plays the same role as translation plays in real space. Some natural processes in nature can be interpreted as a change from one composition C1 to another C2 through the application of a perturbation: p ⊕ C1 ====&gt; C2. The difference between the new observation and the closest TN composition can be back-transformed to the compositional space. The obtained vector is the perturbation vector. closest = closest[clrNo] clr_diff = closest - misbalanced clr_diff ## clrN clrP clrK clrCa clrMg clrFv ## 1540 -0.3757826 -0.3694021 0.4499487 -0.6926226 0.39385 0.5940087 comp_names &lt;- c(&quot;N&quot;, &quot;P&quot;, &quot;K&quot;, &quot;Ca&quot;, &quot;Mg&quot;, &quot;Fv&quot;) perturbation_vector &lt;- clrInv(clr_diff) names(perturbation_vector) &lt;- comp_names Compute the compositions of the ilr coordinates of the misbalanced point, as well as the closest TN point: misbal_comp &lt;- clrInv(misbalanced) names(misbal_comp) &lt;- comp_names closest_comp &lt;- clrInv(closest) names(closest_comp) &lt;- comp_names pmc = rbind(perturbation_vector, misbal_comp, closest_comp) rownames(pmc) = c(&quot;perturbation_vector&quot;,&quot;misbal_comp&quot;,&quot;closest_comp&quot;) pmc # data frame made up of perturbation vector, misbalanced composition and the closest reference sample ## N P K Ca Mg ## perturbation_vector 0.10188730 0.102539477 0.2326648 0.07421954 0.2199719 ## misbal_comp 0.12828046 0.007881602 0.0470000 0.07860000 0.0052000 ## closest_comp 0.05713207 0.003532686 0.0478000 0.02550000 0.0050000 ## Fv ## perturbation_vector 0.2687170 ## misbal_comp 0.7330379 ## closest_comp 0.8610352 For each vector, check that the simplex is closed to 1. sum(perturbation_vector); sum(misbal_comp); sum(closest_comp) ## [1] 1 ## [1] 1 ## [1] 1 The closest composition minus the misbalanced composition should return the perturbation vector. print(closest_comp - misbal_comp) # soustraction ## N P K Ca Mg Fv ## 1540 0.1018873 0.1025395 0.2326648 0.07421954 0.2199719 0.268717 ## attr(,&quot;class&quot;) ## [1] acomp print(perturbation_vector) # for comparison ## N P K Ca Mg Fv ## 1540 0.1018873 0.1025395 0.2326648 0.07421954 0.2199719 0.268717 ## attr(,&quot;class&quot;) ## [1] acomp Or even, perturb the misbalanced point by the perturbation vector, you should obtain the closest TN point: print(misbal_comp + perturbation_vector) # perturbation ## N P K Ca Mg Fv ## 1605 0.05713207 0.003532686 0.0478 0.0255 0.005 0.8610352 ## attr(,&quot;class&quot;) ## [1] acomp print(closest_comp) # for comparison ## N P K Ca Mg Fv ## 1540 0.05713207 0.003532686 0.0478 0.0255 0.005 0.8610352 ## attr(,&quot;class&quot;) ## [1] acomp So, the assumption is true. The next codes try to show the concept using plots using clr coordinates. d = data.frame(rbind(misbalanced, closest, clr_diff)) # mis, cl, per vectors = c(&quot;observation&quot;, &quot;reference&quot;, &quot;perturbation&quot;) d$vectors = factor(vectors) d ## clrN clrP clrK clrCa clrMg clrFv ## 1605 0.9576980 -1.8319897 -0.04637332 0.4678508 -2.247862 2.7006765 ## 1540 0.5819154 -2.2013919 0.40357540 -0.2247718 -1.854012 3.2946852 ## 15401 -0.3757826 -0.3694021 0.44994872 -0.6926226 0.393850 0.5940087 ## vectors ## 1605 observation ## 1540 reference ## 15401 perturbation This data frame must be reshape for ggplot. require(&quot;reshape&quot;) # function melt() dreshape = melt(d) dreshape ## vectors variable value ## 1 observation clrN 0.95769802 ## 2 reference clrN 0.58191537 ## 3 perturbation clrN -0.37578265 ## 4 observation clrP -1.83198972 ## 5 reference clrP -2.20139186 ## 6 perturbation clrP -0.36940213 ## 7 observation clrK -0.04637332 ## 8 reference clrK 0.40357540 ## 9 perturbation clrK 0.44994872 ## 10 observation clrCa 0.46785078 ## 11 reference clrCa -0.22477179 ## 12 perturbation clrCa -0.69262257 ## 13 observation clrMg -2.24786229 ## 14 reference clrMg -1.85401233 ## 15 perturbation clrMg 0.39384997 ## 16 observation clrFv 2.70067654 ## 17 reference clrFv 3.29468520 ## 18 perturbation clrFv 0.59400866 Plot with dots for each vector. options(repr.plot.width = 6, repr.plot.height = 3) ggplot(data = dreshape, aes(x = value, y = vectors)) + geom_point() + facet_wrap(~ variable, scales = &quot;free_x&quot;) + labs(x=&#39;clr coordinate&#39;, y =&#39;&#39;) + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) Figure 4.1: Perturbation vector computation example dotplot using the most imbalanced foliar sample. #ggsave(&quot;images/perturb_dotplot.tiff&quot;, width = 6, height = 4) options(repr.plot.width = 5, repr.plot.height = 3) ggplot(data=dreshape, aes(x=variable, y=value, fill=vectors)) + geom_bar(stat=&quot;identity&quot;, position=position_dodge()) + coord_flip() + theme_bw() + theme(legend.title=element_blank()) + theme(text=element_text(family=&quot;Arial&quot;, face=&quot;bold&quot;, size=12)) Figure 4.2: Perturbation vector computation example barplot using the most imbalanced foliar sample. #ggsave(&quot;images/perturb_barplot.tiff&quot;, width = 6, height = 4) "],
["references.html", "References", " References "]
]
